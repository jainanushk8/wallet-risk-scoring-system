# ðŸŽ¯ Wallet Risk Scoring Methodology

## Overview

This document provides a comprehensive technical explanation of the **Wallet Risk Scoring System** methodology, including data collection strategies, feature engineering approaches, and risk scoring algorithms.

## Table of Contents

1. [System Architecture](#system-architecture)
2. [Data Collection Methodology](#data-collection-methodology)
3. [Feature Engineering Framework](#feature-engineering-framework)
4. [Risk Scoring Model](#risk-scoring-model)
5. [Model Validation](#model-validation)
6. [Technical Decisions](#technical-decisions)
7. [Performance Metrics](#performance-metrics)

---

## System Architecture

### Overall Design Philosophy

The Wallet Risk Scoring System follows a **modular, pipeline-based architecture** designed for:
- **Scalability**: Process 100+ wallets efficiently
- **Maintainability**: Clear separation of concerns
- **Testability**: Each component independently testable
- **Extensibility**: Easy to add new risk factors or data sources

---

## Data Collection Methodology

### Multi-Source Data Strategy

Our system employs a **redundant, multi-source approach** to ensure data completeness and reliability:

#### Primary Data Sources

1. **Etherscan API**
   - **Purpose**: Complete transaction history
   - **Data**: All Ethereum transactions for target wallets
   - **Rate Limit**: 5 calls/second (free tier compliant)
   - **Advantages**: Comprehensive, reliable, real-time

2. **The Graph Protocol**
   - **Purpose**: Compound-specific protocol data
   - **Data**: DeFi positions, lending/borrowing activities
   - **Advantages**: Structured DeFi data, fast queries

3. **Compound API**
   - **Purpose**: Protocol-specific metrics
   - **Data**: Interest rates, market data, protocol statistics
   - **Advantages**: Official protocol data, high accuracy

### Data Collection Process

def collect_wallet_data(wallet_address):
"""
Multi-source data collection with fallback mechanisms
"""
# Step 1: Fetch all transactions from Etherscan
all_transactions = etherscan_api.get_transactions(wallet_address)
# Step 2: Filter Compound-specific transactions
compound_transactions = filter_compound_transactions(all_transactions)

# Step 3: Enrich with The Graph data
compound_positions = the_graph_api.get_compound_data(wallet_address)

# Step 4: Validate and merge data sources
return merge_and_validate_data(all_transactions, compound_transactions, compound_positions)

### Data Quality Assurance

#### Validation Mechanisms

1. **Address Validation**
   - Ethereum address format verification (42 characters, hexadecimal)
   - Checksum validation using Web3 utilities
   - Invalid address filtering (98.1% validation rate achieved)

2. **Transaction Data Validation**
   - Timestamp consistency checks
   - Value range validation (non-negative values)
   - Gas usage reasonableness checks
   - Duplicate transaction removal

3. **Data Completeness Assessment**
data_quality_levels = {
'high': 'Complete transaction history + Compound positions',
'medium': 'Transaction history available, limited Compound data',
'low': 'Minimal transaction data',
'none': 'No transaction data available'
}

---

## Feature Engineering Framework

### Risk Category Design

The feature engineering system extracts **36+ risk indicators** across three main categories:

#### 1. Behavioral Risk Features (40% weight)

**Rationale**: Transaction patterns reveal user behavior and risk-taking tendencies.

##### Transaction Frequency Analysis
def analyze_transaction_frequency(transactions):
return {
'avg_tx_per_day': total_transactions / time_span_days,
'tx_frequency_score': min(1.0, avg_tx_per_day / 5.0),
'recent_activity_ratio': recent_txs / total_transactions,
'burst_activity_score': (time_gaps < 1_hour).sum() / total_gaps
}

##### Key Behavioral Indicators
- **Transaction Frequency**: `avg_tx_per_day`, `recent_activity_ratio`
- **Transaction Sizes**: `avg_tx_value`, `large_tx_ratio`, `tx_value_std`
- **Compound Activity**: `compound_activity_ratio`, `supply_tx_ratio`
- **Temporal Patterns**: `tx_regularity`, `burst_activity_score`

#### 2. Liquidity Risk Features (35% weight)

**Rationale**: Liquidity risk directly impacts likelihood of liquidation events.

##### Collateral Analysis
def calculate_liquidation_risk(collateral_ratio):
if collateral_ratio == 0:
return 1.0 # Maximum risk (no borrowing)
elif collateral_ratio < 1.2: # <120% collateralized
return 0.9 # Critical risk
elif collateral_ratio < 1.5: # <150% collateralized
return 0.7 # High risk
elif collateral_ratio < 2.0: # <200% collateralized
return 0.4 # Medium risk
else:
return 0.1 # Low risk

##### Key Liquidity Indicators
- **Collateral Safety**: `collateral_ratio`, `liquidation_risk_score`
- **Leverage Usage**: `utilization_ratio`, `leverage_score`
- **Position Analysis**: `total_supplied_value`, `total_borrowed_value`

#### 3. Market Risk Features (25% weight)

**Rationale**: Portfolio composition and diversification affect overall risk exposure.

##### Asset Risk Classification
asset_risk_mapping = {
'ETH': 0.6, # Moderate volatility
'DAI': 0.2, # Low risk stablecoin
'USDC': 0.2, # Low risk stablecoin
'USDT': 0.3, # Slightly higher stablecoin risk
'WBTC': 0.7, # High volatility
'COMP': 0.8, # Very high volatility (governance token)
'UNI': 0.8 # Very high volatility (governance token)
}

##### Key Market Indicators
- **Portfolio Risk**: `portfolio_risk_score`, `weighted_asset_risk`
- **Concentration**: `concentration_risk`, `max_single_exposure`
- **Diversification**: `asset_diversification`, `active_asset_count`

### Feature Normalization

All features are normalized to [0, 1] scale before scoring:

def normalize_features(features):
"""
Min-Max normalization with domain-specific bounds
"""
normalization_bounds = {
'transaction_frequency': (0, 50), # 0-50 transactions/day
'collateral_ratio': (0, 5), # 0-500% collateralization
'concentration_risk': (0, 1), # 0-100% concentration
'portfolio_risk': (0, 1) # 0-100% risk assets
}

text
return apply_min_max_scaling(features, normalization_bounds)
text

---

## Risk Scoring Model

### Weighted Composite Scoring Methodology

#### Model Architecture

The risk scoring system uses a **transparent, interpretable weighted composite model**:

def calculate_risk_score(features):
"""
Weighted composite risk scoring
"""
# Extract component scores (normalized 0-1)
behavioral_score = calculate_behavioral_score(features['behavioral'])
liquidity_score = calculate_liquidity_score(features['liquidity'])
market_score = calculate_market_score(features['market'])

# Apply category weights
composite_score = (
    behavioral_score * 0.40 +     # 40% behavioral weight
    liquidity_score * 0.35 +      # 35% liquidity weight
    market_score * 0.25           # 25% market weight
)

# Scale to 0-1000 range
final_score = int(composite_score * 1000)

return np.clip(final_score, 0, 1000)

#### Weight Justification

| Category | Weight | Justification |
|----------|--------|---------------|
| **Behavioral Risk** | 40% | Transaction patterns are the strongest predictor of user risk behavior. High-frequency trading, large transaction volatility, and irregular patterns indicate higher risk tolerance. |
| **Liquidity Risk** | 35% | Direct indicator of liquidation probability. Collateral ratios and leverage usage have immediate impact on position safety. |
| **Market Risk** | 25% | Portfolio composition affects exposure to market volatility. While important, it's partially captured in behavioral patterns. |

### Score Interpretation

| Score Range | Risk Level | Interpretation | Action |
|-------------|------------|----------------|---------|
| **0-250** | Low Risk | Conservative behavior, safe collateral ratios, diversified portfolio | Preferred borrower |
| **251-750** | Medium Risk | Moderate activity, acceptable leverage, some concentration | Standard monitoring |
| **751-1000** | High Risk | Aggressive trading, high leverage, concentrated positions | Enhanced monitoring |

### Data Quality Adjustment

Risk scores are adjusted based on data completeness:

quality_adjustments = {
'high': 1.0, # Complete data - no adjustment
'medium': 0.9, # Some missing data - slight penalty
'low': 0.7, # Limited data - moderate penalty
'none': 0.5 # No useful data - significant penalty
}

adjusted_score = base_score * quality_adjustment_factor

---

## Model Validation

### Statistical Validation

#### Score Distribution Analysis

Our model produces well-calibrated scores with realistic distribution:

Production Results (101 wallets):
â”œâ”€â”€ Mean Score: 272.3
â”œâ”€â”€ Standard Deviation: 65.81
â”œâ”€â”€ Score Range: 183-485
â””â”€â”€ Distribution:
â”œâ”€â”€ Low Risk (0-250): 42 wallets (41.6%)
â”œâ”€â”€ Medium Risk (251-750): 59 wallets (58.4%)
â””â”€â”€ High Risk (751-1000): 0 wallets (0.0%)

#### Key Validation Metrics

1. **No Extreme Outliers**: All scores within reasonable bounds (183-485)
2. **Proper Differentiation**: Clear ranking between wallets
3. **Realistic Distribution**: Majority in medium-risk category
4. **Statistical Stability**: Consistent results across multiple runs

### Component Testing

Each system component achieves **100% test pass rate**:

Feature Engineering Tests
1. Initialization: PASSED
2. Sample Data Extraction: PASSED
3. Empty Data Handling: PASSED
4. Data Collector Integration: PASSED
5. Feature Validation: PASSED

Risk Scoring Tests
1. Initialization: PASSED
2. Score Calculation: PASSED
3. Score Explanation: PASSED
4. Batch Scoring: PASSED
5. Edge Cases: PASSED
6. Scoring Consistency: PASSED


### Real-World Validation

#### Wallet Behavior Analysis

**Highest Risk Wallets (Top 3)**:
- `0x4814be...`: Score 485 - High transaction frequency, active DeFi user
- `0x427f2a...`: Score 451 - Complex transaction patterns
- `0x70d8e4...`: Score 448 - Moderate leverage usage

**Lowest Risk Wallets (Top 3)**:
- `0xa7e94d...`: Score 183 - Minimal activity, conservative behavior
- `0x7851bd...`: Score 200 - Low transaction frequency
- `0x7b4636...`: Score 201 - Simple transaction patterns

---

## Technical Decisions

### Why Weighted Composite Over Machine Learning?

#### Decision Matrix

| Factor | Weighted Composite | Machine Learning | Decision |
|--------|-------------------|------------------|----------|
| **Interpretability** | Fully transparent | Black box | Composite |
| **Data Requirements** | Works with 101 wallets | Needs 1000+ labeled samples | Composite |
| **Training Time** | Instant scoring | Hours of training | Composite |
| **Domain Knowledge** | Incorporates financial theory | Learns patterns only | Composite |
| **Regulatory Compliance** | Explainable decisions | Hard to justify | Composite |
| **Production Deployment** | Lightweight, fast | Complex infrastructure | Composite |

#### When ML Would Be Preferred

Machine Learning would be superior with:
- **Large labeled dataset** (10,000+ wallets with known outcomes)
- **Historical liquidation data** for supervised learning
- **Complex non-linear patterns** that rules can't capture
- **Continuous adaptation** requirements for changing market conditions

### Architecture Decisions

#### Modular Design
- **Rationale**: Each component (data collection, feature engineering, scoring) can be developed, tested, and maintained independently
- **Benefit**: Easy to extend with new data sources or risk factors

#### Multi-Source Data Collection
- **Rationale**: Single API failure doesn't break the entire system
- **Benefit**: Higher data completeness and reliability

#### CPU Optimization
- **Rationale**: System requirements specified no GPU access
- **Benefit**: Runs efficiently on standard hardware

---

## Performance Metrics

### System Performance

| Metric | Target | Achieved | Status |
|--------|--------|----------|---------|
| **Processing Speed** | <10 sec/wallet | 4-8 sec/wallet | Exceeded |
| **Success Rate** | >95% | 100% (101/101) | Exceeded |
| **Memory Usage** | <2GB | <1GB | Exceeded |
| **API Compliance** | 5 calls/sec | 5 calls/sec | Met |

### Data Quality Metrics

| Metric | Result |
|--------|--------|
| **Address Validation Rate** | 98.1% (101/103 valid) |
| **Data Completeness** | 100% transaction data, 30% Compound positions |
| **Error Rate** | 0% (no processing failures) |
| **Feature Extraction Success** | 100% (all wallets processed) |

### Score Quality Metrics

| Metric | Value |
|--------|-------|
| **Score Range Utilization** | 302 points (183-485) |
| **Standard Deviation** | 65.81 (good differentiation) |
| **Outlier Rate** | 0% (no extreme scores) |
| **Distribution Balance** | 42% low, 58% medium, 0% high |

---

## Future Enhancements

### Potential Improvements

1. **Machine Learning Integration**
Hybrid approach: Composite baseline + ML refinement
baseline_score = weighted_composite_score(features)
ml_adjustment = trained_model.predict(features) if sufficient_data else 0
final_score = 0.8 * baseline_score + 0.2 * ml_adjustment

2. **Real-Time Market Data**
- Dynamic asset risk weights based on current volatility
- Market correlation adjustments
- Liquidity pool health monitoring

3. **Advanced Risk Factors**
- Cross-protocol activity analysis
- Governance token voting behavior
- MEV (Maximum Extractable Value) exposure

4. **Performance Optimization**
- Parallel processing for large wallet batches
- Caching mechanisms for repeated queries
- Database integration for historical tracking

### Scalability Roadmap

- **Phase 1**: Current system (100+ wallets) Complete
- **Phase 2**: 1,000+ wallets with database backend
- **Phase 3**: 10,000+ wallets with distributed processing
- **Phase 4**: Real-time risk monitoring system

---

## Conclusion

The **Wallet Risk Scoring System** successfully delivers a production-ready solution for DeFi wallet risk assessment. Key achievements:

**100% Success Rate**: All 101 wallets processed without errors  
**Well-Calibrated Scores**: Realistic distribution without extreme outliers  
**Transparent Methodology**: Fully interpretable weighted composite approach  
**Enterprise Architecture**: Modular, testable, maintainable design  
**Professional Documentation**: Comprehensive technical specification  

The system demonstrates that **rule-based approaches can be highly effective** for financial risk assessment when properly designed with domain knowledge and statistical rigor.

---

*For technical questions or implementation details, please refer to the source code documentation or create an issue in the GitHub repository.*